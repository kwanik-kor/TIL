# 1. 컴퓨터 구조의 큰 그림


---
# 2. 컴퓨터가 이해하는 정보
> 0과 1만 이해하는 컴퓨터가 문자와 숫자를 인식하는 방법, 표현된 정적인 데이터가 명령어에 대해 어떻게 실행되는지

- 비트(bit): 정보를 표현하는 가장 작은 단위
- 워드(word): CPU 관점에서의 정보 단위로, CPU가 한 번에 처리할 수 있는 데이터의 크기
	- CPU가 한 번에 32비트를 처리할 수 있다면 1워드 = 32비트

## 2.1 데이터 - 0과 1로 숫자 표현하기

- CPU는 0과 1만 이해할 수 있으며 컴퓨터는 2진법을 사용
- **소수 표현**
	- 표현하고자 하는 소수와 실제로 저장된 소수 간의 오차가 발생할 수 있음
	- **부동 소수점(IEEE 754)**
		- 필요에 따라 소수점의 위치가 이동할 수 있음
		- m x 2^n
			- m : 가수(significant)
			- n : 지수(exponent)
		- 부호 비트 + 지수 비트 + 가수 비트
			- 32 비트 : 1 + 8 + 23
			- 64 비트 : 1 + 11 + 52
		- 가수는 1.XXX 형태로 1로 **정규화한 수를 저장**
		- 1.1010111010101 x 2^6
			- 지수 : 바이어스(bias) + 6
				- **바이어스**: 2^(k - 1) - 1
			- 가수 : 1010111010101 저장
```Java
var a = 0.1;  
var b = 0.2;  
var c = 0.3;  
  
if (a + b == c) {  
    System.out.println("equal");  
} else {
	// here works!
    System.out.println("not equal");  
}
```

## 2.2 데이터 - 0과 1로 문자 표현하기

- 문자 집합(Character set): 컴퓨터가 이해할 수 있는 문자들의 집합
	- 문자 인코딩(Character Encoding): 문자 집합 속 문자를 0과 1로 이루어진 문자 코드로 변환하는 과정
	- 문자 디코딩(Character Decoding): 0과 1로 표현된 문자를 사람이 이해하는 문자로 변환하는 과정

- **ASCII(American Standard Code for Information Interchange)**
	- 영어, 알파벳, 아라비아 숫자, 일부 특수문자
	- 8비트(1 Byte)
		- 그 중 1 비트는 **패리티 비트**로 오류 검출용 비트
		- 실질적으로는 7비트로써 128개의 문자 표현
	- 코드 포인트(Code Point)
		- 글자에 부여된 고유한 값
		- A : 65
		- a : 97
	- 한글 표현 X

- **EUC-KR**
	- 아스키 문자 : 1 바이트
	- 하나의 한글 글자 : 2 바이트
		- 네 자리 16진수로 표현
	- 

## 2.3 명령어

---
# 3. CPU

# 4. 메모리

# 5. 보조기억장치와 입출력장치